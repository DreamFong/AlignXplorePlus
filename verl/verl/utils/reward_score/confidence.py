# Copyright 2024 Bytedance Ltd. and/or its affiliates
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import re
import json
import math
import random

from verl.utils.reward_score.math import is_equiv
from openai import OpenAI


urls = {
    # "Qwen3-8B-h2011": "http://33.184.124.23:8000/v1",
    "Qwen3-8B-h2018": "http://33.184.125.114:8000/v1",
    "Qwen3-8B-h200": "http://33.181.225.31:8000/v1",
}
client = OpenAI(
    base_url=urls["Qwen3-8B-h200"],
    api_key="EMPTY",
)
with open(
    "/ossfs/workspace/nas/yuting/code/PreInf/reinforcement learning/playground/preference.txt",
    "r",
    encoding="utf-8",
) as f:
    system_prompt = f.read()


# def extract_preference(response):
#     pattern = re.compile(r"<answer>(.*?)</answer>", re.DOTALL)
#     matches = re.findall(pattern, response)
#     if matches:
#         result = matches[-1].replace("answer here", "")
#     else:
#         result = ""

#     return result


# def extract_preference(response):
#     pattern = re.compile(r"<answer>(.*?)(?:</answer>|$)", re.DOTALL)
#     matches = re.findall(pattern, response)
#     if matches:
#         result = matches[-1].replace("answer here", "").strip()
#     else:
#         result = ""
#     return result


def extract_preference(response):
    return response.strip().split("</think>")[-1].strip()


def format_score(output_string):
    if not (
        output_string.count("<think>") == 1 and output_string.count("</think>") == 1
    ):
        return False
    pattern = re.compile(r"^<think>.+<\/think>.+$", re.DOTALL)
    # 或者 re.S
    if pattern.match(output_string):
        return True
    return False


# def _reward_template(reward_info, response_str):
#     task = reward_info["task"]
#     responseA = reward_info["responseA"]
#     responseB = reward_info["responseB"]
#     preference = extract_preference(response_str)

#     prop = f'Determine which response the user prefers based on the user’s preferences. Please output your selection below in a json format by filling in the placeholders in []:{{"selection": "[Response A / Response B]"}}\n{system_prompt}\n<Prompt>\n{task}\n</Prompt>\n\n<Preference>\n{preference}</Preference>\n\n<Response A>\n{responseA}\n</Response A>\n\n<Response B>\n{responseB}\n</Response B>\n\n'
#     messages = [
#         {"role": "system", "content": "Generate a task-specific response."},
#         {"role": "user", "content": prop},
#     ]

#     return messages


# def _reward_template_with_answer(reward_info, response_str, ground_truth):
#     def extract_preference(response):
#         import re

#         pattern = re.compile(r"<answer>(.*?)</answer>", re.DOTALL)
#         matches = re.findall(pattern, response)
#         if matches:
#             result = matches[-1].replace("answer here", "")
#         else:
#             result = ""

#         return result

#     task = reward_info["task"]
#     responseA = reward_info["responseA"]
#     responseB = reward_info["responseB"]
#     preference = extract_preference(response_str)

#     prop = f'Determine which response the user prefers based on the user’s preferences. Please output your selection below in a json format by filling in the placeholders in []:{{"selection": "[Response A / Response B]"}}\n<Prompt>\n{task}\n</Prompt>\n\n<Preference>\n{preference}</Preference>\n\n<Response A>\n{responseA}\n</Response A>\n\n<Response B>\n{responseB}\n</Response B>\n\n{{"selection": "{ground_truth}"}}'

#     return prop


def _reward_template_upi(post, preference):
    chosen = post["chosen"]
    rejected = post["rejected"]
    task = post["query"]
    flag = random.randint(0, 1)
    if flag:
        responseA = chosen
        responseB = rejected
        answer = "Response A"
    else:
        responseA = rejected
        responseB = chosen
        answer = "Response B"

    prop = f'Determine which response the user prefers based on the user’s preferences. Please output your selection below in a json format by filling in the placeholders in []:{{"selection": "[Response A / Response B]"}}\n{system_prompt}\n<Prompt>\n{task}\n</Prompt>\n\n<Preference>\n{preference}</Preference>\n\n<Response A>\n{responseA}\n</Response A>\n\n<Response B>\n{responseB}\n</Response B>\n\n'
    messages = [
        {"role": "system", "content": "Generate a task-specific response."},
        {"role": "user", "content": prop},
    ]

    return messages, answer


def generate_final_answer(messages):
    models = client.models.list()
    model = models.data[0].id
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        max_tokens=64,
        stream=False,  # 在此模板中，我们等待完整响应
        extra_body={"chat_template_kwargs": {"enable_thinking": False}},
        logprobs=True,
    )
    return response.choices[0].message.content, response.choices[0].logprobs.content


# def generate_confidence_without_new_token(prompt):
#     models = client.models.list()
#     model = models.data[0].id
#     response = client.completions.create(
#         model=model,
#         prompt=prompt,
#         max_tokens=1,
#         stream=False,  # 在此模板中，我们等待完整响应
#         extra_body={
#             "enable_thinking": 0,
#             "prompt_logprobs": 1,
#         },
#     )
#     return response.choices[0].prompt_logprobs


def extract_final_answers(response: str) -> str:
    try:
        res = json.loads(response)
        res = res["selection"]
    except:
        try:
            match = re.search(r"\{.*\}", response)
            if match:
                json_str = match.group()
                parsed_json = json.loads(json_str)
                res = parsed_json
                res = res["selection"]
            else:
                res = response
        except:
            res = response

    return res


def extract_confidence(logprobs_content, answer):
    def normalize_token(tok: str) -> str:
        return tok.replace("Ġ", " ").replace("Ċ", "\n")

    confidence_of_A = None
    confidence_of_B = None
    # 打印所有 token 的信息，便于调试
    # print("\n--- 所有生成Token的详细信息 ---")
    # for item in logprobs_content:
    #     # breakpoint()
    #     print(f"Token: '{item.token}', LogProb: {item.logprob:.4f}")

    for item in logprobs_content:
        # 注意：Token可能包含前导空格，例如 " A"。使用 strip() 来处理这种情况。
        if normalize_token(item.token.strip()) == "A":
            logprob_of_A = item.logprob
            # 第4步：转换成标准概率 (0-1)
            confidence_of_A = math.exp(logprob_of_A)
        if normalize_token(item.token.strip()) == "B":
            logprob_of_B = item.logprob
            confidence_of_B = math.exp(logprob_of_B)
    if confidence_of_A is None and confidence_of_B is None:
        return -1.0
    if answer == "Response A":
        confidence_of_answer = confidence_of_A or (1 - confidence_of_B)
    elif answer == "Response B":
        confidence_of_answer = confidence_of_B or (1 - confidence_of_A)
    return confidence_of_answer


def extract_confidence_with_answer(prompt_logprobs, answer):
    # print(prompt_logprobs)
    target_char = answer.split()[1]  # 'A' or 'B'
    target_token_logprob = None

    for token_options in reversed(prompt_logprobs):
        # 跳过列表开头的 null
        if not token_options:
            continue

        # 3. 在当前位置的所有 token 可能性中查找
        for _, token_data in token_options.items():
            # 4. 检查 decoded_token 是否完全匹配目标
            if token_data["decoded_token"].strip() == target_char:
                target_token_logprob = token_data["logprob"]
    if target_token_logprob is not None:
        confidence = math.exp(target_token_logprob)
    else:
        print(
            f"======== Error: {prompt_logprobs} ======= \n Tagert answer: {target_char}"
        )
        raise AssertionError("Ground truth token not found in prompt logprobs.")
    return confidence


# def compute_score(solution_str, ground_truth=None, data_source=None, extra_info=None):
#     """The scoring function for AlingXplore.

#     Args:
#         solution_str: the solution text
#         ground_truth: the ground truth
#     """
#     reward_prompt = _reward_template(extra_info, solution_str)
#     reward_response, logprobs = generate_final_answer(reward_prompt)
#     if reward_response is None:
#         raise ValueError("======== API receives none content. ========")
#     reward = extract_confidence(logprobs, ground_truth)
#     return reward


# def compute_score(solution_str, ground_truth, data_source=None, extra_info=None):
#     """The scoring function for AlingXplore.

#     Args:
#         solution_str: the solution text
#         ground_truth: the ground truth
#     """
#     reward_prompt = _reward_template_with_answer(extra_info, solution_str, ground_truth)
#     logprobs = generate_confidence_without_new_token(reward_prompt)
#     if logprobs is None:
#         raise ValueError("======== API receives none content. ========")
#     reward = extract_confidence_with_answer(logprobs, ground_truth)
#     return reward


def compute_score(solution_str, ground_truth=None, data_source=None, extra_info=None):
    """The scoring function for AlingXplore.

    Args:
        solution_str: the solution text
        ground_truth: the ground truth
    """
    post = extra_info["target2"]
    if not format_score(solution_str):
        return {
            "score": 0.0,
            "format_socre": -1.0,
            "confidence": 0.0,
        }
    preference = extract_preference(solution_str)
    reward_prompt, answer = _reward_template_upi(post, preference)
    reward_response, logprobs = generate_final_answer(reward_prompt)
    if reward_response is None:
        raise ValueError("======== API receives none content. ========")
    reward = extract_confidence(logprobs, answer)
    return {
        "score": reward,
        "format_socre": 0.0,
        "confidence": reward,
    }
